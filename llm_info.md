## LLM de Código Abierto Identificados (de DataCamp)

### 1. LLaMA 3.1
- **Parámetros:** 8B, 70B, 405B.
- **Capacidades:** Maneja diversas tareas de procesamiento de lenguaje natural en varios idiomas (inglés, español, portugués, alemán, tailandés, francés, italiano, hindi). Soporta una longitud de contexto de 128.000 tokens. El modelo 405B es potente para la generación de datos sintéticos y destilación de conocimientos.
- **Licencia/Acceso:** No especificado directamente en el texto, pero se infiere que es de código abierto.

### 2. BLOOM
- **Parámetros:** 176.000 millones.
- **Capacidades:** LLM autorregresivo entrenado para continuar texto a partir de un prompt. Proporciona textos coherentes y precisos en 46 lenguas y 13 lenguajes de programación.
- **Licencia/Acceso:** Código abierto, acceso gratuito a través del ecosistema Hugging Face.

### 3. BERT
- **Parámetros:** No especificado, pero existen miles de modelos preentrenados.
- **Capacidades:** Desarrollado por Google, alcanzó un rendimiento puntero en muchas tareas de procesamiento del lenguaje natural. Utilizado en la Búsqueda de Google. Modelos específicos para análisis de sentimientos, análisis de notas clínicas, detección de comentarios tóxicos.
- **Licencia/Acceso:** Código abierto, gratuito.

### 4. Falcon 180B
- **Parámetros:** 180.000 millones, entrenado con 3,5 billones de fichas.
- **Capacidades:** Superó a LLaMA 2 y GPT-3.5 en varias tareas de PLN. Puede rivalizar con PaLM 2 de Google. Requiere importantes recursos informáticos.
- **Licencia/Acceso:** Gratuito para uso comercial y de investigación.

### 5. OPT-175B
- **Parámetros:** 125M a 175B (OPT-175B es el más potente).
- **Capacidades:** Conjunto de transformadores preentrenados solo para descodificador. Rendimiento similar al GPT-3.
- **Licencia/Acceso:** Código abierto, modelos preentrenados y código fuente disponibles públicamente.

### 6. XGen-7B
- **Parámetros:** 7B.
- **Capacidades:** Admite ventanas contextuales más largas (hasta 8K tokens). Eficiente en el uso de parámetros.
- **Licencia/Acceso:** Disponible para fines comerciales y de investigación, excepto la variante XGen-7B-{4K,8K}-inst (licencia no comercial).

### 7. GPT-NeoX y GPT-J
- **Parámetros:** GPT-NeoX (20.000 millones), GPT-J (6.000 millones).
- **Capacidades:** Entrenados con 22 conjuntos de datos de alta calidad, permiten su uso en múltiples dominios y casos de uso.
- **Licencia/Acceso:** Disponibles gratuitamente a través de la API de NLP Cloud.

### 8. Vicuña 13-B
- **Parámetros:** 13B.
- **Capacidades:** Modelo conversacional entrenado a partir del ajuste fino de LLaMa 13B. Calidad superior al 90% de ChatGPT y Google Bard en evaluaciones preliminares.
- **Licencia/Acceso:** Código abierto.

### 9. Mistral Small
- **Parámetros:** 24B.
- **Capacidades:** Centrado en tareas de razonamiento avanzado, lógica de cadena de pensamiento y pasos múltiples en varios idiomas.
- **Licencia/Acceso:** Licencia Apache 2.0, acceso a través de Hugging Face.



## LLM Seleccionados para el Agente de IA

Basado en la investigación, se han seleccionado los siguientes cinco LLM de código abierto y gratuitos por sus fortalezas diferenciadas:

1.  **LLaMA 3.1:** Elegido por su capacidad multilingüe, su gran tamaño de parámetros (405B) para tareas avanzadas como la generación de datos sintéticos y la destilación de conocimientos, y su amplia ventana de contexto.

2.  **BLOOM:** Seleccionado por su robustez en la generación de texto coherente en múltiples idiomas y lenguajes de programación, y su naturaleza completamente transparente y de código abierto a través de Hugging Face.

3.  **Falcon 180B:** Incluido por su impresionante rendimiento en tareas de PLN, superando a otros modelos populares como LLaMA 2 y GPT-3.5, y su disponibilidad para uso comercial y de investigación, a pesar de sus altos requisitos de recursos.

4.  **Vicuña 13-B:** Destacado por su enfoque conversacional y su alta calidad en la generación de diálogos, con un rendimiento comparable a ChatGPT y Google Bard en evaluaciones preliminares, lo que lo hace ideal para interacciones de usuario.

5.  **Mistral Small:** Elegido por su especialización en tareas de razonamiento avanzado, lógica de cadena de pensamiento y pasos múltiples en varios idiomas, lo que lo hace valioso para la mejora de la precisión y la calidad de las respuestas complejas.

